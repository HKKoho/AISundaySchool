var l={};async function c(e){try{const o=await fetch("/api/chat",{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({provider:"ollama",model:e.model,messages:e.messages,temperature:e.temperature,topP:e.topP,maxTokens:2e3})});if(!o.ok){const r=await o.text();throw new Error(`Ollama Cloud API error (${o.status}): ${r}`)}const t=await o.json();if(t.content)return{content:t.content,model:t.model};throw new Error("Invalid response format from Ollama Cloud API")}catch(o){throw console.error("Ollama Cloud API error:",o),new Error(`Failed to get response: ${o.message}`)}}async function m(e){try{const o=await fetch("/api/chat",{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({provider:"openai",model:e.model,messages:e.messages,temperature:e.temperature,topP:e.topP,maxTokens:2e3})});if(!o.ok){const r=await o.text();throw new Error(`OpenAI API error (${o.status}): ${r}`)}const t=await o.json();if(t.content)return{content:t.content,model:t.model};throw new Error("Invalid response format from OpenAI API")}catch(o){throw console.error("OpenAI API error:",o),new Error(`Failed to get response from OpenAI: ${o.message}`)}}async function p(e){const o=l.GEMINI_API_KEY||"";if(!o)throw new Error("GEMINI_API_KEY is not configured. Please add it to .env.local to use Gemini models.");try{const t=e.messages.filter(n=>n.role!=="system").map(n=>({role:n.role==="assistant"?"model":"user",parts:[{text:n.content}]})),r=e.messages.find(n=>n.role==="system"),a=await fetch(`https://generativelanguage.googleapis.com/v1beta/models/${e.model}:generateContent?key=${o}`,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({contents:t,systemInstruction:r?{parts:[{text:r.content}]}:void 0,generationConfig:{temperature:e.temperature,topP:e.topP,maxOutputTokens:2e3}})});if(!a.ok){const n=await a.text();throw new Error(`Gemini API error (${a.status}): ${n}`)}const s=await a.json();if(s.candidates&&s.candidates[0]&&s.candidates[0].content)return{content:s.candidates[0].content.parts.map(i=>i.text).join(""),model:e.model};throw new Error("Invalid response format from Gemini API")}catch(t){throw console.error("Gemini API error:",t),new Error(`Failed to get response from Gemini: ${t.message}`)}}async function d(e){const o="http://localhost:11434";try{const t=await fetch(`${o}/api/chat`,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({model:e.model,messages:e.messages,stream:!1,options:{temperature:e.temperature,top_p:e.topP,num_predict:2e3}})});if(!t.ok)throw new Error(`Local Ollama error: ${t.status}`);const r=await t.json();if(r.message&&r.message.content)return{content:r.message.content,model:e.model};throw new Error("Invalid response from local Ollama")}catch(t){throw console.error("Local Ollama error:",t),new Error(`無法連接到本地 Ollama 服務。請確保：
1. Ollama 已安裝並正在運行
2. 模型已下載 (ollama pull ${e.model})
3. 服務運行在 http://localhost:11434`)}}const h=["kimi-k2:1t","qwen3-coder:480b","deepseek-v3.1:671b","gpt-oss:120b","gpt-oss:20b"];async function f(e){const o=e.model;return h.includes(o)?c(e):o.startsWith("gpt-")?m(e):o.startsWith("gemini-")?p(e):d(e)}function g(e){const o=`你是一位專業的神學研究助手，擁有深厚的聖經知識、教會歷史和系統神學理解。你的回應應該：

1. 基於聖經真理和正統神學傳統
2. 提供準確的經文引用和歷史背景
3. 以學術嚴謹但易於理解的方式表達
4. 尊重不同的神學立場，但明確指出你的觀點基礎
5. 鼓勵深入思考和屬靈成長

請用繁體中文回應。`,t={"Theology Chat":"當前模式：神學對話。請針對用戶的神學問題提供深入且平衡的回答。","Reading Q&A":"當前模式：文檔問答。請基於已上傳的文檔內容回答問題，並提供相關的引用和分析。","Assignment Assistant":"當前模式：作業助手。請幫助用戶完成神學作業，提供學術性的指導和建議。","Resource Search":"當前模式：資源搜尋。請幫助用戶找到相關的神學資源和參考文獻。"};return`${o}

${t[e]||t["Theology Chat"]}`}export{g as createTheologySystemPrompt,f as sendChatMessage};

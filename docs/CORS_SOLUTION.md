# CORS Solution: API Proxy Architecture

## Problem Explained

### What is CORS?
**CORS (Cross-Origin Resource Sharing)** is a security feature in web browsers that blocks JavaScript from making requests to a different domain than the one serving the webpage.

### Why Did the APIs Fail?

1. **Ollama Cloud API** (`https://ollama.com`)
   - ‚ùå No CORS headers for browser requests
   - ‚úÖ Works from backend (Node.js, curl)
   - ‚ùå Browser blocks: "Response to preflight request doesn't pass access control check"

2. **OpenAI API** (`https://api.openai.com`)
   - ‚ùå No CORS headers for browser requests
   - ‚úÖ Works from backend
   - ‚ùå Browser blocks: Same CORS error

3. **Gemini API** (`https://generativelanguage.googleapis.com`)
   - ‚úÖ Has CORS headers
   - ‚úÖ Works from browser directly

## Solution: Vite Dev Server Proxy

### Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Browser   ‚îÇ         ‚îÇ  Vite Dev Server ‚îÇ         ‚îÇ  External APIs  ‚îÇ
‚îÇ             ‚îÇ         ‚îÇ   (localhost)    ‚îÇ         ‚îÇ                 ‚îÇ
‚îÇ  Frontend   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ                  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  ollama.com     ‚îÇ
‚îÇ  JavaScript ‚îÇ  /api/* ‚îÇ  Proxy Layer     ‚îÇ  HTTPS  ‚îÇ  openai.com     ‚îÇ
‚îÇ             ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  (adds headers)  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  gemini (direct)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### How It Works

1. **Frontend makes request to `/api/ollama`**
   ```javascript
   fetch('/api/ollama', {
     method: 'POST',
     body: JSON.stringify({ model, messages, ... })
   });
   ```

2. **Vite proxy intercepts the request**
   - Same origin (localhost ‚Üí localhost): No CORS issue ‚úÖ
   - Proxy adds `Authorization` header with API key
   - Proxy forwards to real API endpoint

3. **External API responds to Vite server**
   - Vite receives response
   - Forwards back to browser
   - Browser sees it as same-origin response ‚úÖ

## Configuration

### `vite.config.ts`
```typescript
server: {
  proxy: {
    '/api/ollama': {
      target: 'https://ollama.com',
      changeOrigin: true,
      rewrite: (path) => path.replace(/^\/api\/ollama/, '/v1/chat/completions'),
      configure: (proxy, _options) => {
        proxy.on('proxyReq', (proxyReq, req, _res) => {
          proxyReq.setHeader('Authorization', `Bearer ${env.OLLAMA_API_KEY}`);
        });
      },
    },
    '/api/openai': {
      target: 'https://api.openai.com',
      changeOrigin: true,
      rewrite: (path) => path.replace(/^\/api\/openai/, '/v1/chat/completions'),
      configure: (proxy, _options) => {
        proxy.on('proxyReq', (proxyReq, req, _res) => {
          proxyReq.setHeader('Authorization', `Bearer ${env.OPENAI_API_KEY}`);
        });
      },
    },
  },
}
```

### Service Code (Updated)
```typescript
// Before (CORS blocked):
const response = await fetch('https://ollama.com/v1/chat/completions', {
  headers: {
    'Authorization': `Bearer ${API_KEY}` // Exposed in browser!
  }
});

// After (via proxy):
const response = await fetch('/api/ollama', {
  headers: {
    'Content-Type': 'application/json'
    // No Authorization header - proxy adds it!
  }
});
```

## Security Benefits

1. **API Keys Hidden from Browser**
   - Keys stay in `.env.local` file
   - Never exposed to frontend JavaScript
   - Cannot be extracted by users or tools

2. **Same-Origin Requests**
   - Browser sees: `localhost:3005/api/ollama`
   - No CORS preflight needed
   - Standard browser security applies

3. **Centralized Authentication**
   - Single place to manage API keys
   - Easy to rotate keys
   - Can add rate limiting or caching

## Files Modified

### Configuration
- ‚úÖ `vite.config.ts` - Added proxy configuration
- ‚úÖ `.env.local` - API keys (already present)

### Services
- ‚úÖ `services/theologyAssistantService.ts`
  - `callOllamaCloud()` - Now uses `/api/ollama`
  - `callOpenAI()` - Now uses `/api/openai`

- ‚úÖ `services/localLLMService.ts`
  - `callOllamaCloud()` - Now uses `/api/ollama`

## Testing

### 1. Test Ollama Cloud
Open Á•ûÂ≠∏Á†îÁ©∂Âä©Êâã, select `gpt-oss:20b`, send a message.

**Expected:**
- ‚úÖ Request goes to `/api/ollama`
- ‚úÖ Vite proxy forwards to `https://ollama.com/v1/chat/completions`
- ‚úÖ Response received without CORS errors

### 2. Test OpenAI
Select `gpt-4o-mini`, send a message.

**Expected:**
- ‚úÖ Request goes to `/api/openai`
- ‚úÖ Vite proxy forwards to `https://api.openai.com/v1/chat/completions`
- ‚úÖ Response received without CORS errors

### 3. Test Gemini
Select `gemini-2.0-flash`, send a message.

**Expected:**
- ‚úÖ Direct request to Google API (has CORS headers)
- ‚úÖ Works as before

## Browser Console

### Before (CORS Errors)
```
‚ùå Access to fetch at 'https://ollama.com/v1/chat/completions' from origin
   'http://localhost:3005' has been blocked by CORS policy
‚ùå Failed to load resource: net::ERR_FAILED
```

### After (Success)
```
‚úÖ POST /api/ollama 200 OK (1.2s)
‚úÖ Response received successfully
```

## Production Deployment

**Important:** Vite proxy only works in development!

For production, you need:

### Option 1: Backend API (Recommended)
Create actual backend endpoints:
- `/api/ollama` ‚Üí Node.js/Express/Vercel Function
- `/api/openai` ‚Üí Same backend

### Option 2: Use Vercel Serverless Functions
Already created: `api/chat.ts`

```typescript
// Vercel function that handles proxy in production
export default async function handler(req, res) {
  // Proxy logic here
}
```

### Option 3: Use a dedicated proxy service
- Services like CORS Anywhere
- Or deploy your own proxy server

## Why This is Better

### Before
- ‚ùå CORS errors
- ‚ùå API keys exposed in browser
- ‚ùå Can't use OpenAI or Ollama Cloud
- ‚ùå Security risk

### After
- ‚úÖ No CORS errors
- ‚úÖ API keys hidden on server
- ‚úÖ All APIs work (Ollama, OpenAI, Gemini)
- ‚úÖ Better security
- ‚úÖ Professional architecture

## Summary

**The Problem:** Browser CORS policy blocked direct API calls to Ollama Cloud and OpenAI.

**The Solution:** Vite development proxy routes requests through localhost, avoiding CORS while keeping API keys secure.

**The Result:** All three AI services now work perfectly in the Theology Assistant! üéâ
